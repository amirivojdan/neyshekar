{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55a0645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in raw data: 20020\n"
     ]
    }
   ],
   "source": [
    "# read raw json data\n",
    "import json\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "dataset_root = pathlib.Path(\"../dataset\")\n",
    "raw_data_path = dataset_root / \"neyshekar_export.json\"\n",
    "\n",
    "with open(raw_data_path, \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# print the number of entries in the raw data\n",
    "print(f\"Number of entries in raw data: {len(raw_data)}\")\n",
    "\n",
    "# shuffle the raw data\n",
    "random.shuffle(raw_data)\n",
    "\n",
    "# go through each record and add and id starting from 0\n",
    "shuffled_data = []\n",
    "for i, entry in enumerate(raw_data):\n",
    "    entry[\"id\"] = i\n",
    "    shuffled_data.append(entry)\n",
    "\n",
    "raw_data = shuffled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "168b655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20020/20020 [00:00<00:00, 182743.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# go through each entry and download the voice files\n",
    "import requests\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "output_dir = dataset_root / \"downloaded_voices\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for entry in tqdm.tqdm(raw_data):\n",
    "    voice_url = entry.get(\"audio_url\")\n",
    "    voice_id = entry.get(\"voice_id\")\n",
    "    if voice_url:\n",
    "        # extract file extension\n",
    "        file_extension = voice_url.split(\".\")[-1].split(\"?\")[0]\n",
    "        filename = f\"{voice_id}.{file_extension}\"\n",
    "        output_path = output_dir / filename\n",
    "\n",
    "        if output_path.exists():\n",
    "            continue\n",
    "\n",
    "        # download the file\n",
    "        response = requests.get(voice_url)\n",
    "        if response.status_code == 200:\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13fc8ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20020it [1:28:37,  3.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# now convert all downloaded files to wav format\n",
    "# Format      : WAV (PCM)\n",
    "# Sample Rate : 16,000 Hz\n",
    "# Bit Depth   : 16-bit\n",
    "# Channels    : Mono\n",
    "\n",
    "import pydub\n",
    "\n",
    "export_dir = dataset_root / \"export\"\n",
    "converted_dir = export_dir / \"audio\"\n",
    "raw_converted_dir = export_dir / \"raw_audio\"\n",
    "os.makedirs(converted_dir, exist_ok=True)\n",
    "os.makedirs(raw_converted_dir, exist_ok=True)\n",
    "\n",
    "for audio_file in tqdm.tqdm(output_dir.iterdir()):\n",
    "    if audio_file.suffix.lower() not in [\".wav\",\".mp4\", \".mp3\", \".ogg\", \".flac\", \".m4a\", \".webm\"]:\n",
    "        continue\n",
    "\n",
    "    # fine the corresponding id in the raw data\n",
    "    entry = next((e for e in raw_data if e[\"voice_id\"] == audio_file.stem), None)\n",
    "    if entry is None:\n",
    "        continue\n",
    "\n",
    "    file_id = entry[\"id\"]\n",
    "    \n",
    "    # file id padded to 6 digits\n",
    "    file_id = str(file_id).zfill(6)\n",
    "\n",
    "    output_path = converted_dir / f\"neyshekar_{file_id}.wav\"\n",
    "    raw_output_path = raw_converted_dir / f\"neyshekar_{file_id}.wav\"\n",
    "\n",
    "    if not raw_output_path.exists():\n",
    "        audio = pydub.AudioSegment.from_file(audio_file)\n",
    "        audio = audio.set_sample_width(2)\n",
    "        audio.export(raw_output_path, format=\"wav\", codec=\"pcm_s16le\", parameters=[\"-map_metadata\", \"-1\"])\n",
    "\n",
    "    if not output_path.exists():\n",
    "        audio = pydub.AudioSegment.from_file(audio_file)\n",
    "        audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
    "        audio.export(output_path, format=\"wav\", codec=\"pcm_s16le\", parameters=[\"-map_metadata\", \"-1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b1fb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2026-01-14 23:56:14.412193340 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n",
      "20020it [00:02, 8741.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from shekar import Normalizer\n",
    "\n",
    "# output a json file with these fields:\n",
    "# id: sequential id starting from 0\n",
    "# audio: filename of the wav file in the audio directory\n",
    "# duration: duration of the audio file in seconds\n",
    "# text: transcription text\n",
    "\n",
    "nomalizer = Normalizer()\n",
    "audio_freq = 16000\n",
    "\n",
    "final_data = []\n",
    " \n",
    "for _, entry in tqdm.tqdm(enumerate(raw_data)):\n",
    "    \n",
    "    id = entry.get(\"id\")\n",
    "    file_id = str(id).zfill(6)\n",
    "    voice_id = f\"neyshekar_{file_id}\"\n",
    "\n",
    "    text = entry.get(\"text_content\", \"\").strip()\n",
    "    # normalize with shekar \n",
    "    text = nomalizer(text)\n",
    "\n",
    "    audio_path = converted_dir / f\"{voice_id}.wav\"\n",
    "\n",
    "    if not audio_path.exists() or len(text) == 0:\n",
    "        continue\n",
    "\n",
    "    audio = pydub.AudioSegment.from_file(audio_path)\n",
    "    \n",
    "    final_data.append({\n",
    "        \"id\": id,\n",
    "        \"audio\": f\"{voice_id}.wav\",\n",
    "        \"text\": text,\n",
    "        \"duration\": audio.duration_seconds,\n",
    "    })\n",
    "\n",
    "final_output_path = dataset_root / \"dataset.json\"\n",
    "with open(final_output_path, \"w\") as f:\n",
    "    json.dump(final_data, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1514f640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 20020\n",
      "Total duration (hours): 29.08\n",
      "Average clip duration (seconds): 5.23\n",
      "Total tokens: 208472\n",
      "Vocab size: 20853\n"
     ]
    }
   ],
   "source": [
    "# now print some statistics about the final data\n",
    "# these are the metrics we care about:\n",
    "# total number of samples\n",
    "# total duration in hours, average duration\n",
    "# total number of tokens in the dataset\n",
    "# vocab size\n",
    "# histogram of durations\n",
    "# histogram of named entities in the text\n",
    "\n",
    "from shekar import WordTokenizer\n",
    "tokenizer = WordTokenizer()\n",
    "\n",
    "total_duration = sum([entry[\"duration\"] for entry in final_data])\n",
    "total_samples = len(final_data)\n",
    "all_text = \" \".join([entry[\"text\"] for entry in final_data])\n",
    "tokens = list(tokenizer.tokenize(all_text))\n",
    "total_tokens = len(tokens)\n",
    "vocab = set(tokens)\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Total duration (hours): {total_duration / 3600:.2f}\")\n",
    "print(f\"Average clip duration (seconds): {total_duration / total_samples:.2f}\")\n",
    "print(f\"Total tokens: {total_tokens}\")\n",
    "print(f\"Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15df133b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# histogram of durations\n",
    "durations = [entry[\"duration\"] for entry in final_data]\n",
    "max_duration = max(durations)\n",
    "plt.hist(durations, bins=int(max_duration), range=(0, int(max_duration)))\n",
    "# each bin represents duration range of 1 second\n",
    "plt.xlabel(\"Duration (seconds)\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.title(\"Histogram of Clip Durations\")\n",
    "plt.savefig(dataset_root / \"duration_histogram.png\")\n",
    "plt.clf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5c61d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20020/20020 [1:15:39<00:00,  4.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from shekar import NER\n",
    "\n",
    "ner = NER()\n",
    "\n",
    "# histogram of named entities in the text\n",
    "all_entities = []\n",
    "for entry in tqdm.tqdm(final_data):\n",
    "    entities = ner(entry[\"text\"])\n",
    "    all_entities.extend(entities)\n",
    "\n",
    "# count occurrences of each entity type\n",
    "\n",
    "all_entities = [entity for text, entity in all_entities]\n",
    "\n",
    "entity_counts = {}\n",
    "for tag in all_entities:\n",
    "    entity_counts[tag] = entity_counts.get(tag, 0) + 1\n",
    "\n",
    "# plot histogram of entity types\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(entity_counts.keys(), entity_counts.values())\n",
    "plt.xlabel(\"Entity Types\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Entity Types\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(dataset_root / \"entity_histogram.png\")\n",
    "plt.clf()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7e1fafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity type counts:\n",
      "ORG: 965\n",
      "DAT: 1913\n",
      "LOC: 2421\n",
      "PER: 998\n",
      "EVE: 139\n"
     ]
    }
   ],
   "source": [
    "# number of entities per entity type\n",
    "\n",
    "entity_type_counts = {}\n",
    "for entity in all_entities:\n",
    "    entity_type_counts[entity] = entity_type_counts.get(entity, 0) + 1\n",
    "print(\"Entity type counts:\")\n",
    "for entity_type, count in entity_type_counts.items():\n",
    "    print(f\"{entity_type}: {count}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neyshekar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
